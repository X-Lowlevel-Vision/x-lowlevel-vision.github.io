<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description"
    content="  Research works about interpreting and explaining low-level vision networks">
  <title> Interpretable Low-Level Vision </title>

  <!-- stylesheets -->
  <link media="all" href="//netdna.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css" rel="stylesheet">
  <link media="all" href="//netdna.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
  <link media="all" rel="stylesheet" href="/assets/css/site.css">

</head>


<body>
  <header>
  <nav class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container-fluid">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a id="site-title" class="navbar-brand" href="/">
          <img class="pull-left img-responsive" src=""> 
          <span class="name-holder">Interpretable Low-Level Vision</span>
        </a>
      </div>

      <div class="collapse navbar-collapse" id="navbar">

        <ul id="main-menu" class="nav navbar-nav navbar-right">

          
            <li><a href="/">
              <i class="fa fa-home"></i> Research</a>
            </li>
          
            <li><a href="/team/">
              <i class="fa fa-info-circle"></i> Team</a>
            </li>
          

        </ul>
      </div>
      <!-- /.navbar-collapse -->
    </div>
    <!-- /.container-fluid -->
  </nav>

</header>

  
  
  <div class="jumbotron blog-header">
  <div class="container">
    <div class="row">
        <div class="centered-text col-xs-12 col-sm-12 col-md-offset-1 col-md-10 col-lg-offset-1 col-lg-10">
            <h1>Interpretable Low-Level Vision</h1>
            <p> Research works about interpreting and explaining low-level vision networks </p>
        </div>
    </div>
  </div>
</div>


<div class="container">
  <div class="row">
    <div class="col-xs-12 col-sm-12 col-md-offset-1 col-md-10 col-lg-offset-2 col-lg-8">
      
        <div class="blog-short">
          <h3><a href="/lam.html">Interpreting Super-Resolution Networks with Local Attribution Maps</a></h3>
          <h5>Computer Vision and Pattern Recognition 2021</h5>
  		<div>
           </div>
          
            <div class="imgcontainer col-xs-12 col-sm-5 col-md-5 col-lg-5">
              <img src="http://placehold.it/300x200"
              alt="Interpreting Super-Resolution Networks with Local Attribution Maps"  class="pull-left img-responsive thumb img-thumbnail" />
            </div>
          
          <article class="excerpt"><p>Image super-resolution (SR) techniques have been developing rapidly, benefiting from the invention of deep networks and its successive breakthroughs. However, it is acknowledged that deep learning and deep neural networks are difficult to interpret. SR networks inherit this mysterious nature and little works make attempt to understand them. In this paper, we perform attribution analysis of SR networks, which aims at finding the input pixels that strongly influence the SR results. We propose a novel attribution approach called local attribution map (LAM), which inherits the integral gradient method yet with two unique features. One is to use the blurred image as the baseline input, and the other is to adopt the progressive blurring function as the path function. Based on LAM, we show that: (1) SR networks with a wider range of involved input pixels could achieve better performance. (2) Attention networks and non-local networks extract features from a wider range of input pixels. (3) Comparing with the range that actually contributes, the receptive field is large enough for most deep networks. (4) For SR networks, textures with regular stripes or grids are more likely to be noticed, while complex semantics are difficult to utilize. Our work opens new directions for designing SR networks and interpreting low-level vision deep models.</p>

</article>
          <a class="pull-right marginBottom10" href="/lam.html">Read More</a>
        </div>
      
    </div>
  </div>
</div>


  

<footer class="footer">
    <div class="container">
      <div class="row">
        <div class="col-lg-offset-3 col-lg-6">
            <div class="row">
              <div class="col-xs-12 text-center">
                <small>&copy; Interpretable Low-Level Vision, 2020</small>
              </div>
            </div>
        </div>
      </div>
    </div>
  </footer>


  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
  <script src="//netdna.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
  <!--<script src="/assets/js/prism.js"></script> -->
  <script src="/assets/js/site.js"></script>

</body>

</html>
